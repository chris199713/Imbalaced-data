{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.model_selection import cross_val_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv('creditcard.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Time</th>\n",
       "      <th>V1</th>\n",
       "      <th>V2</th>\n",
       "      <th>V3</th>\n",
       "      <th>V4</th>\n",
       "      <th>V5</th>\n",
       "      <th>V6</th>\n",
       "      <th>V7</th>\n",
       "      <th>V8</th>\n",
       "      <th>V9</th>\n",
       "      <th>...</th>\n",
       "      <th>V21</th>\n",
       "      <th>V22</th>\n",
       "      <th>V23</th>\n",
       "      <th>V24</th>\n",
       "      <th>V25</th>\n",
       "      <th>V26</th>\n",
       "      <th>V27</th>\n",
       "      <th>V28</th>\n",
       "      <th>Amount</th>\n",
       "      <th>Class</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.0</td>\n",
       "      <td>-1.359807</td>\n",
       "      <td>-0.072781</td>\n",
       "      <td>2.536347</td>\n",
       "      <td>1.378155</td>\n",
       "      <td>-0.338321</td>\n",
       "      <td>0.462388</td>\n",
       "      <td>0.239599</td>\n",
       "      <td>0.098698</td>\n",
       "      <td>0.363787</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.018307</td>\n",
       "      <td>0.277838</td>\n",
       "      <td>-0.110474</td>\n",
       "      <td>0.066928</td>\n",
       "      <td>0.128539</td>\n",
       "      <td>-0.189115</td>\n",
       "      <td>0.133558</td>\n",
       "      <td>-0.021053</td>\n",
       "      <td>149.62</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.0</td>\n",
       "      <td>1.191857</td>\n",
       "      <td>0.266151</td>\n",
       "      <td>0.166480</td>\n",
       "      <td>0.448154</td>\n",
       "      <td>0.060018</td>\n",
       "      <td>-0.082361</td>\n",
       "      <td>-0.078803</td>\n",
       "      <td>0.085102</td>\n",
       "      <td>-0.255425</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.225775</td>\n",
       "      <td>-0.638672</td>\n",
       "      <td>0.101288</td>\n",
       "      <td>-0.339846</td>\n",
       "      <td>0.167170</td>\n",
       "      <td>0.125895</td>\n",
       "      <td>-0.008983</td>\n",
       "      <td>0.014724</td>\n",
       "      <td>2.69</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1.0</td>\n",
       "      <td>-1.358354</td>\n",
       "      <td>-1.340163</td>\n",
       "      <td>1.773209</td>\n",
       "      <td>0.379780</td>\n",
       "      <td>-0.503198</td>\n",
       "      <td>1.800499</td>\n",
       "      <td>0.791461</td>\n",
       "      <td>0.247676</td>\n",
       "      <td>-1.514654</td>\n",
       "      <td>...</td>\n",
       "      <td>0.247998</td>\n",
       "      <td>0.771679</td>\n",
       "      <td>0.909412</td>\n",
       "      <td>-0.689281</td>\n",
       "      <td>-0.327642</td>\n",
       "      <td>-0.139097</td>\n",
       "      <td>-0.055353</td>\n",
       "      <td>-0.059752</td>\n",
       "      <td>378.66</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1.0</td>\n",
       "      <td>-0.966272</td>\n",
       "      <td>-0.185226</td>\n",
       "      <td>1.792993</td>\n",
       "      <td>-0.863291</td>\n",
       "      <td>-0.010309</td>\n",
       "      <td>1.247203</td>\n",
       "      <td>0.237609</td>\n",
       "      <td>0.377436</td>\n",
       "      <td>-1.387024</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.108300</td>\n",
       "      <td>0.005274</td>\n",
       "      <td>-0.190321</td>\n",
       "      <td>-1.175575</td>\n",
       "      <td>0.647376</td>\n",
       "      <td>-0.221929</td>\n",
       "      <td>0.062723</td>\n",
       "      <td>0.061458</td>\n",
       "      <td>123.50</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>2.0</td>\n",
       "      <td>-1.158233</td>\n",
       "      <td>0.877737</td>\n",
       "      <td>1.548718</td>\n",
       "      <td>0.403034</td>\n",
       "      <td>-0.407193</td>\n",
       "      <td>0.095921</td>\n",
       "      <td>0.592941</td>\n",
       "      <td>-0.270533</td>\n",
       "      <td>0.817739</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.009431</td>\n",
       "      <td>0.798278</td>\n",
       "      <td>-0.137458</td>\n",
       "      <td>0.141267</td>\n",
       "      <td>-0.206010</td>\n",
       "      <td>0.502292</td>\n",
       "      <td>0.219422</td>\n",
       "      <td>0.215153</td>\n",
       "      <td>69.99</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>284802</th>\n",
       "      <td>172786.0</td>\n",
       "      <td>-11.881118</td>\n",
       "      <td>10.071785</td>\n",
       "      <td>-9.834783</td>\n",
       "      <td>-2.066656</td>\n",
       "      <td>-5.364473</td>\n",
       "      <td>-2.606837</td>\n",
       "      <td>-4.918215</td>\n",
       "      <td>7.305334</td>\n",
       "      <td>1.914428</td>\n",
       "      <td>...</td>\n",
       "      <td>0.213454</td>\n",
       "      <td>0.111864</td>\n",
       "      <td>1.014480</td>\n",
       "      <td>-0.509348</td>\n",
       "      <td>1.436807</td>\n",
       "      <td>0.250034</td>\n",
       "      <td>0.943651</td>\n",
       "      <td>0.823731</td>\n",
       "      <td>0.77</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>284803</th>\n",
       "      <td>172787.0</td>\n",
       "      <td>-0.732789</td>\n",
       "      <td>-0.055080</td>\n",
       "      <td>2.035030</td>\n",
       "      <td>-0.738589</td>\n",
       "      <td>0.868229</td>\n",
       "      <td>1.058415</td>\n",
       "      <td>0.024330</td>\n",
       "      <td>0.294869</td>\n",
       "      <td>0.584800</td>\n",
       "      <td>...</td>\n",
       "      <td>0.214205</td>\n",
       "      <td>0.924384</td>\n",
       "      <td>0.012463</td>\n",
       "      <td>-1.016226</td>\n",
       "      <td>-0.606624</td>\n",
       "      <td>-0.395255</td>\n",
       "      <td>0.068472</td>\n",
       "      <td>-0.053527</td>\n",
       "      <td>24.79</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>284804</th>\n",
       "      <td>172788.0</td>\n",
       "      <td>1.919565</td>\n",
       "      <td>-0.301254</td>\n",
       "      <td>-3.249640</td>\n",
       "      <td>-0.557828</td>\n",
       "      <td>2.630515</td>\n",
       "      <td>3.031260</td>\n",
       "      <td>-0.296827</td>\n",
       "      <td>0.708417</td>\n",
       "      <td>0.432454</td>\n",
       "      <td>...</td>\n",
       "      <td>0.232045</td>\n",
       "      <td>0.578229</td>\n",
       "      <td>-0.037501</td>\n",
       "      <td>0.640134</td>\n",
       "      <td>0.265745</td>\n",
       "      <td>-0.087371</td>\n",
       "      <td>0.004455</td>\n",
       "      <td>-0.026561</td>\n",
       "      <td>67.88</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>284805</th>\n",
       "      <td>172788.0</td>\n",
       "      <td>-0.240440</td>\n",
       "      <td>0.530483</td>\n",
       "      <td>0.702510</td>\n",
       "      <td>0.689799</td>\n",
       "      <td>-0.377961</td>\n",
       "      <td>0.623708</td>\n",
       "      <td>-0.686180</td>\n",
       "      <td>0.679145</td>\n",
       "      <td>0.392087</td>\n",
       "      <td>...</td>\n",
       "      <td>0.265245</td>\n",
       "      <td>0.800049</td>\n",
       "      <td>-0.163298</td>\n",
       "      <td>0.123205</td>\n",
       "      <td>-0.569159</td>\n",
       "      <td>0.546668</td>\n",
       "      <td>0.108821</td>\n",
       "      <td>0.104533</td>\n",
       "      <td>10.00</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>284806</th>\n",
       "      <td>172792.0</td>\n",
       "      <td>-0.533413</td>\n",
       "      <td>-0.189733</td>\n",
       "      <td>0.703337</td>\n",
       "      <td>-0.506271</td>\n",
       "      <td>-0.012546</td>\n",
       "      <td>-0.649617</td>\n",
       "      <td>1.577006</td>\n",
       "      <td>-0.414650</td>\n",
       "      <td>0.486180</td>\n",
       "      <td>...</td>\n",
       "      <td>0.261057</td>\n",
       "      <td>0.643078</td>\n",
       "      <td>0.376777</td>\n",
       "      <td>0.008797</td>\n",
       "      <td>-0.473649</td>\n",
       "      <td>-0.818267</td>\n",
       "      <td>-0.002415</td>\n",
       "      <td>0.013649</td>\n",
       "      <td>217.00</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>284807 rows × 31 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "            Time         V1         V2        V3        V4        V5  \\\n",
       "0            0.0  -1.359807  -0.072781  2.536347  1.378155 -0.338321   \n",
       "1            0.0   1.191857   0.266151  0.166480  0.448154  0.060018   \n",
       "2            1.0  -1.358354  -1.340163  1.773209  0.379780 -0.503198   \n",
       "3            1.0  -0.966272  -0.185226  1.792993 -0.863291 -0.010309   \n",
       "4            2.0  -1.158233   0.877737  1.548718  0.403034 -0.407193   \n",
       "...          ...        ...        ...       ...       ...       ...   \n",
       "284802  172786.0 -11.881118  10.071785 -9.834783 -2.066656 -5.364473   \n",
       "284803  172787.0  -0.732789  -0.055080  2.035030 -0.738589  0.868229   \n",
       "284804  172788.0   1.919565  -0.301254 -3.249640 -0.557828  2.630515   \n",
       "284805  172788.0  -0.240440   0.530483  0.702510  0.689799 -0.377961   \n",
       "284806  172792.0  -0.533413  -0.189733  0.703337 -0.506271 -0.012546   \n",
       "\n",
       "              V6        V7        V8        V9  ...       V21       V22  \\\n",
       "0       0.462388  0.239599  0.098698  0.363787  ... -0.018307  0.277838   \n",
       "1      -0.082361 -0.078803  0.085102 -0.255425  ... -0.225775 -0.638672   \n",
       "2       1.800499  0.791461  0.247676 -1.514654  ...  0.247998  0.771679   \n",
       "3       1.247203  0.237609  0.377436 -1.387024  ... -0.108300  0.005274   \n",
       "4       0.095921  0.592941 -0.270533  0.817739  ... -0.009431  0.798278   \n",
       "...          ...       ...       ...       ...  ...       ...       ...   \n",
       "284802 -2.606837 -4.918215  7.305334  1.914428  ...  0.213454  0.111864   \n",
       "284803  1.058415  0.024330  0.294869  0.584800  ...  0.214205  0.924384   \n",
       "284804  3.031260 -0.296827  0.708417  0.432454  ...  0.232045  0.578229   \n",
       "284805  0.623708 -0.686180  0.679145  0.392087  ...  0.265245  0.800049   \n",
       "284806 -0.649617  1.577006 -0.414650  0.486180  ...  0.261057  0.643078   \n",
       "\n",
       "             V23       V24       V25       V26       V27       V28  Amount  \\\n",
       "0      -0.110474  0.066928  0.128539 -0.189115  0.133558 -0.021053  149.62   \n",
       "1       0.101288 -0.339846  0.167170  0.125895 -0.008983  0.014724    2.69   \n",
       "2       0.909412 -0.689281 -0.327642 -0.139097 -0.055353 -0.059752  378.66   \n",
       "3      -0.190321 -1.175575  0.647376 -0.221929  0.062723  0.061458  123.50   \n",
       "4      -0.137458  0.141267 -0.206010  0.502292  0.219422  0.215153   69.99   \n",
       "...          ...       ...       ...       ...       ...       ...     ...   \n",
       "284802  1.014480 -0.509348  1.436807  0.250034  0.943651  0.823731    0.77   \n",
       "284803  0.012463 -1.016226 -0.606624 -0.395255  0.068472 -0.053527   24.79   \n",
       "284804 -0.037501  0.640134  0.265745 -0.087371  0.004455 -0.026561   67.88   \n",
       "284805 -0.163298  0.123205 -0.569159  0.546668  0.108821  0.104533   10.00   \n",
       "284806  0.376777  0.008797 -0.473649 -0.818267 -0.002415  0.013649  217.00   \n",
       "\n",
       "        Class  \n",
       "0           0  \n",
       "1           0  \n",
       "2           0  \n",
       "3           0  \n",
       "4           0  \n",
       "...       ...  \n",
       "284802      0  \n",
       "284803      0  \n",
       "284804      0  \n",
       "284805      0  \n",
       "284806      0  \n",
       "\n",
       "[284807 rows x 31 columns]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "display(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\User\\AppData\\Local\\Temp\\ipykernel_8836\\270786113.py:2: FutureWarning: \n",
      "\n",
      "Passing `palette` without assigning `hue` is deprecated and will be removed in v0.14.0. Assign the `x` variable to `hue` and set `legend=False` for the same effect.\n",
      "\n",
      "  ax = sns.countplot(x = 'Class', data = df, palette = 'pastel')\n"
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 432x576 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.figure(figsize = (6,8))\n",
    "ax = sns.countplot(x = 'Class', data = df, palette = 'pastel')\n",
    "for count in ax.containers:\n",
    "    ax.bar_label(count,)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Genuine: 99.82725143693798 %\n",
      "fraud: 0.1727485630620034 %\n"
     ]
    }
   ],
   "source": [
    "neg_per = df['Class'].value_counts()[0] / len(df) * 100\n",
    "pos_per = df['Class'].value_counts()[1] / len(df) * 100\n",
    "print(f'Genuine: {neg_per} %')\n",
    "print(f'fraud: {pos_per} %')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>V1</th>\n",
       "      <th>V2</th>\n",
       "      <th>V3</th>\n",
       "      <th>V4</th>\n",
       "      <th>V5</th>\n",
       "      <th>V6</th>\n",
       "      <th>V7</th>\n",
       "      <th>V8</th>\n",
       "      <th>V9</th>\n",
       "      <th>V10</th>\n",
       "      <th>...</th>\n",
       "      <th>V21</th>\n",
       "      <th>V22</th>\n",
       "      <th>V23</th>\n",
       "      <th>V24</th>\n",
       "      <th>V25</th>\n",
       "      <th>V26</th>\n",
       "      <th>V27</th>\n",
       "      <th>V28</th>\n",
       "      <th>Amount</th>\n",
       "      <th>Class</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>-1.359807</td>\n",
       "      <td>-0.072781</td>\n",
       "      <td>2.536347</td>\n",
       "      <td>1.378155</td>\n",
       "      <td>-0.338321</td>\n",
       "      <td>0.462388</td>\n",
       "      <td>0.239599</td>\n",
       "      <td>0.098698</td>\n",
       "      <td>0.363787</td>\n",
       "      <td>0.090794</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.018307</td>\n",
       "      <td>0.277838</td>\n",
       "      <td>-0.110474</td>\n",
       "      <td>0.066928</td>\n",
       "      <td>0.128539</td>\n",
       "      <td>-0.189115</td>\n",
       "      <td>0.133558</td>\n",
       "      <td>-0.021053</td>\n",
       "      <td>149.62</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1.191857</td>\n",
       "      <td>0.266151</td>\n",
       "      <td>0.166480</td>\n",
       "      <td>0.448154</td>\n",
       "      <td>0.060018</td>\n",
       "      <td>-0.082361</td>\n",
       "      <td>-0.078803</td>\n",
       "      <td>0.085102</td>\n",
       "      <td>-0.255425</td>\n",
       "      <td>-0.166974</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.225775</td>\n",
       "      <td>-0.638672</td>\n",
       "      <td>0.101288</td>\n",
       "      <td>-0.339846</td>\n",
       "      <td>0.167170</td>\n",
       "      <td>0.125895</td>\n",
       "      <td>-0.008983</td>\n",
       "      <td>0.014724</td>\n",
       "      <td>2.69</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>-1.358354</td>\n",
       "      <td>-1.340163</td>\n",
       "      <td>1.773209</td>\n",
       "      <td>0.379780</td>\n",
       "      <td>-0.503198</td>\n",
       "      <td>1.800499</td>\n",
       "      <td>0.791461</td>\n",
       "      <td>0.247676</td>\n",
       "      <td>-1.514654</td>\n",
       "      <td>0.207643</td>\n",
       "      <td>...</td>\n",
       "      <td>0.247998</td>\n",
       "      <td>0.771679</td>\n",
       "      <td>0.909412</td>\n",
       "      <td>-0.689281</td>\n",
       "      <td>-0.327642</td>\n",
       "      <td>-0.139097</td>\n",
       "      <td>-0.055353</td>\n",
       "      <td>-0.059752</td>\n",
       "      <td>378.66</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>-0.966272</td>\n",
       "      <td>-0.185226</td>\n",
       "      <td>1.792993</td>\n",
       "      <td>-0.863291</td>\n",
       "      <td>-0.010309</td>\n",
       "      <td>1.247203</td>\n",
       "      <td>0.237609</td>\n",
       "      <td>0.377436</td>\n",
       "      <td>-1.387024</td>\n",
       "      <td>-0.054952</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.108300</td>\n",
       "      <td>0.005274</td>\n",
       "      <td>-0.190321</td>\n",
       "      <td>-1.175575</td>\n",
       "      <td>0.647376</td>\n",
       "      <td>-0.221929</td>\n",
       "      <td>0.062723</td>\n",
       "      <td>0.061458</td>\n",
       "      <td>123.50</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>-1.158233</td>\n",
       "      <td>0.877737</td>\n",
       "      <td>1.548718</td>\n",
       "      <td>0.403034</td>\n",
       "      <td>-0.407193</td>\n",
       "      <td>0.095921</td>\n",
       "      <td>0.592941</td>\n",
       "      <td>-0.270533</td>\n",
       "      <td>0.817739</td>\n",
       "      <td>0.753074</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.009431</td>\n",
       "      <td>0.798278</td>\n",
       "      <td>-0.137458</td>\n",
       "      <td>0.141267</td>\n",
       "      <td>-0.206010</td>\n",
       "      <td>0.502292</td>\n",
       "      <td>0.219422</td>\n",
       "      <td>0.215153</td>\n",
       "      <td>69.99</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>284802</th>\n",
       "      <td>-11.881118</td>\n",
       "      <td>10.071785</td>\n",
       "      <td>-9.834783</td>\n",
       "      <td>-2.066656</td>\n",
       "      <td>-5.364473</td>\n",
       "      <td>-2.606837</td>\n",
       "      <td>-4.918215</td>\n",
       "      <td>7.305334</td>\n",
       "      <td>1.914428</td>\n",
       "      <td>4.356170</td>\n",
       "      <td>...</td>\n",
       "      <td>0.213454</td>\n",
       "      <td>0.111864</td>\n",
       "      <td>1.014480</td>\n",
       "      <td>-0.509348</td>\n",
       "      <td>1.436807</td>\n",
       "      <td>0.250034</td>\n",
       "      <td>0.943651</td>\n",
       "      <td>0.823731</td>\n",
       "      <td>0.77</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>284803</th>\n",
       "      <td>-0.732789</td>\n",
       "      <td>-0.055080</td>\n",
       "      <td>2.035030</td>\n",
       "      <td>-0.738589</td>\n",
       "      <td>0.868229</td>\n",
       "      <td>1.058415</td>\n",
       "      <td>0.024330</td>\n",
       "      <td>0.294869</td>\n",
       "      <td>0.584800</td>\n",
       "      <td>-0.975926</td>\n",
       "      <td>...</td>\n",
       "      <td>0.214205</td>\n",
       "      <td>0.924384</td>\n",
       "      <td>0.012463</td>\n",
       "      <td>-1.016226</td>\n",
       "      <td>-0.606624</td>\n",
       "      <td>-0.395255</td>\n",
       "      <td>0.068472</td>\n",
       "      <td>-0.053527</td>\n",
       "      <td>24.79</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>284804</th>\n",
       "      <td>1.919565</td>\n",
       "      <td>-0.301254</td>\n",
       "      <td>-3.249640</td>\n",
       "      <td>-0.557828</td>\n",
       "      <td>2.630515</td>\n",
       "      <td>3.031260</td>\n",
       "      <td>-0.296827</td>\n",
       "      <td>0.708417</td>\n",
       "      <td>0.432454</td>\n",
       "      <td>-0.484782</td>\n",
       "      <td>...</td>\n",
       "      <td>0.232045</td>\n",
       "      <td>0.578229</td>\n",
       "      <td>-0.037501</td>\n",
       "      <td>0.640134</td>\n",
       "      <td>0.265745</td>\n",
       "      <td>-0.087371</td>\n",
       "      <td>0.004455</td>\n",
       "      <td>-0.026561</td>\n",
       "      <td>67.88</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>284805</th>\n",
       "      <td>-0.240440</td>\n",
       "      <td>0.530483</td>\n",
       "      <td>0.702510</td>\n",
       "      <td>0.689799</td>\n",
       "      <td>-0.377961</td>\n",
       "      <td>0.623708</td>\n",
       "      <td>-0.686180</td>\n",
       "      <td>0.679145</td>\n",
       "      <td>0.392087</td>\n",
       "      <td>-0.399126</td>\n",
       "      <td>...</td>\n",
       "      <td>0.265245</td>\n",
       "      <td>0.800049</td>\n",
       "      <td>-0.163298</td>\n",
       "      <td>0.123205</td>\n",
       "      <td>-0.569159</td>\n",
       "      <td>0.546668</td>\n",
       "      <td>0.108821</td>\n",
       "      <td>0.104533</td>\n",
       "      <td>10.00</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>284806</th>\n",
       "      <td>-0.533413</td>\n",
       "      <td>-0.189733</td>\n",
       "      <td>0.703337</td>\n",
       "      <td>-0.506271</td>\n",
       "      <td>-0.012546</td>\n",
       "      <td>-0.649617</td>\n",
       "      <td>1.577006</td>\n",
       "      <td>-0.414650</td>\n",
       "      <td>0.486180</td>\n",
       "      <td>-0.915427</td>\n",
       "      <td>...</td>\n",
       "      <td>0.261057</td>\n",
       "      <td>0.643078</td>\n",
       "      <td>0.376777</td>\n",
       "      <td>0.008797</td>\n",
       "      <td>-0.473649</td>\n",
       "      <td>-0.818267</td>\n",
       "      <td>-0.002415</td>\n",
       "      <td>0.013649</td>\n",
       "      <td>217.00</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>283726 rows × 30 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "               V1         V2        V3        V4        V5        V6  \\\n",
       "0       -1.359807  -0.072781  2.536347  1.378155 -0.338321  0.462388   \n",
       "1        1.191857   0.266151  0.166480  0.448154  0.060018 -0.082361   \n",
       "2       -1.358354  -1.340163  1.773209  0.379780 -0.503198  1.800499   \n",
       "3       -0.966272  -0.185226  1.792993 -0.863291 -0.010309  1.247203   \n",
       "4       -1.158233   0.877737  1.548718  0.403034 -0.407193  0.095921   \n",
       "...           ...        ...       ...       ...       ...       ...   \n",
       "284802 -11.881118  10.071785 -9.834783 -2.066656 -5.364473 -2.606837   \n",
       "284803  -0.732789  -0.055080  2.035030 -0.738589  0.868229  1.058415   \n",
       "284804   1.919565  -0.301254 -3.249640 -0.557828  2.630515  3.031260   \n",
       "284805  -0.240440   0.530483  0.702510  0.689799 -0.377961  0.623708   \n",
       "284806  -0.533413  -0.189733  0.703337 -0.506271 -0.012546 -0.649617   \n",
       "\n",
       "              V7        V8        V9       V10  ...       V21       V22  \\\n",
       "0       0.239599  0.098698  0.363787  0.090794  ... -0.018307  0.277838   \n",
       "1      -0.078803  0.085102 -0.255425 -0.166974  ... -0.225775 -0.638672   \n",
       "2       0.791461  0.247676 -1.514654  0.207643  ...  0.247998  0.771679   \n",
       "3       0.237609  0.377436 -1.387024 -0.054952  ... -0.108300  0.005274   \n",
       "4       0.592941 -0.270533  0.817739  0.753074  ... -0.009431  0.798278   \n",
       "...          ...       ...       ...       ...  ...       ...       ...   \n",
       "284802 -4.918215  7.305334  1.914428  4.356170  ...  0.213454  0.111864   \n",
       "284803  0.024330  0.294869  0.584800 -0.975926  ...  0.214205  0.924384   \n",
       "284804 -0.296827  0.708417  0.432454 -0.484782  ...  0.232045  0.578229   \n",
       "284805 -0.686180  0.679145  0.392087 -0.399126  ...  0.265245  0.800049   \n",
       "284806  1.577006 -0.414650  0.486180 -0.915427  ...  0.261057  0.643078   \n",
       "\n",
       "             V23       V24       V25       V26       V27       V28  Amount  \\\n",
       "0      -0.110474  0.066928  0.128539 -0.189115  0.133558 -0.021053  149.62   \n",
       "1       0.101288 -0.339846  0.167170  0.125895 -0.008983  0.014724    2.69   \n",
       "2       0.909412 -0.689281 -0.327642 -0.139097 -0.055353 -0.059752  378.66   \n",
       "3      -0.190321 -1.175575  0.647376 -0.221929  0.062723  0.061458  123.50   \n",
       "4      -0.137458  0.141267 -0.206010  0.502292  0.219422  0.215153   69.99   \n",
       "...          ...       ...       ...       ...       ...       ...     ...   \n",
       "284802  1.014480 -0.509348  1.436807  0.250034  0.943651  0.823731    0.77   \n",
       "284803  0.012463 -1.016226 -0.606624 -0.395255  0.068472 -0.053527   24.79   \n",
       "284804 -0.037501  0.640134  0.265745 -0.087371  0.004455 -0.026561   67.88   \n",
       "284805 -0.163298  0.123205 -0.569159  0.546668  0.108821  0.104533   10.00   \n",
       "284806  0.376777  0.008797 -0.473649 -0.818267 -0.002415  0.013649  217.00   \n",
       "\n",
       "        Class  \n",
       "0           0  \n",
       "1           0  \n",
       "2           0  \n",
       "3           0  \n",
       "4           0  \n",
       "...       ...  \n",
       "284802      0  \n",
       "284803      0  \n",
       "284804      0  \n",
       "284805      0  \n",
       "284806      0  \n",
       "\n",
       "[283726 rows x 30 columns]"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.drop_duplicates(inplace = True)\n",
    "df.drop('Time', axis = 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Time</th>\n",
       "      <th>V1</th>\n",
       "      <th>V2</th>\n",
       "      <th>V3</th>\n",
       "      <th>V4</th>\n",
       "      <th>V5</th>\n",
       "      <th>V6</th>\n",
       "      <th>V7</th>\n",
       "      <th>V8</th>\n",
       "      <th>V9</th>\n",
       "      <th>...</th>\n",
       "      <th>V21</th>\n",
       "      <th>V22</th>\n",
       "      <th>V23</th>\n",
       "      <th>V24</th>\n",
       "      <th>V25</th>\n",
       "      <th>V26</th>\n",
       "      <th>V27</th>\n",
       "      <th>V28</th>\n",
       "      <th>Amount</th>\n",
       "      <th>Class</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.0</td>\n",
       "      <td>-1.359807</td>\n",
       "      <td>-0.072781</td>\n",
       "      <td>2.536347</td>\n",
       "      <td>1.378155</td>\n",
       "      <td>-0.338321</td>\n",
       "      <td>0.462388</td>\n",
       "      <td>0.239599</td>\n",
       "      <td>0.098698</td>\n",
       "      <td>0.363787</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.018307</td>\n",
       "      <td>0.277838</td>\n",
       "      <td>-0.110474</td>\n",
       "      <td>0.066928</td>\n",
       "      <td>0.128539</td>\n",
       "      <td>-0.189115</td>\n",
       "      <td>0.133558</td>\n",
       "      <td>-0.021053</td>\n",
       "      <td>0.244200</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.0</td>\n",
       "      <td>1.191857</td>\n",
       "      <td>0.266151</td>\n",
       "      <td>0.166480</td>\n",
       "      <td>0.448154</td>\n",
       "      <td>0.060018</td>\n",
       "      <td>-0.082361</td>\n",
       "      <td>-0.078803</td>\n",
       "      <td>0.085102</td>\n",
       "      <td>-0.255425</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.225775</td>\n",
       "      <td>-0.638672</td>\n",
       "      <td>0.101288</td>\n",
       "      <td>-0.339846</td>\n",
       "      <td>0.167170</td>\n",
       "      <td>0.125895</td>\n",
       "      <td>-0.008983</td>\n",
       "      <td>0.014724</td>\n",
       "      <td>-0.342584</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1.0</td>\n",
       "      <td>-1.358354</td>\n",
       "      <td>-1.340163</td>\n",
       "      <td>1.773209</td>\n",
       "      <td>0.379780</td>\n",
       "      <td>-0.503198</td>\n",
       "      <td>1.800499</td>\n",
       "      <td>0.791461</td>\n",
       "      <td>0.247676</td>\n",
       "      <td>-1.514654</td>\n",
       "      <td>...</td>\n",
       "      <td>0.247998</td>\n",
       "      <td>0.771679</td>\n",
       "      <td>0.909412</td>\n",
       "      <td>-0.689281</td>\n",
       "      <td>-0.327642</td>\n",
       "      <td>-0.139097</td>\n",
       "      <td>-0.055353</td>\n",
       "      <td>-0.059752</td>\n",
       "      <td>1.158900</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1.0</td>\n",
       "      <td>-0.966272</td>\n",
       "      <td>-0.185226</td>\n",
       "      <td>1.792993</td>\n",
       "      <td>-0.863291</td>\n",
       "      <td>-0.010309</td>\n",
       "      <td>1.247203</td>\n",
       "      <td>0.237609</td>\n",
       "      <td>0.377436</td>\n",
       "      <td>-1.387024</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.108300</td>\n",
       "      <td>0.005274</td>\n",
       "      <td>-0.190321</td>\n",
       "      <td>-1.175575</td>\n",
       "      <td>0.647376</td>\n",
       "      <td>-0.221929</td>\n",
       "      <td>0.062723</td>\n",
       "      <td>0.061458</td>\n",
       "      <td>0.139886</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>2.0</td>\n",
       "      <td>-1.158233</td>\n",
       "      <td>0.877737</td>\n",
       "      <td>1.548718</td>\n",
       "      <td>0.403034</td>\n",
       "      <td>-0.407193</td>\n",
       "      <td>0.095921</td>\n",
       "      <td>0.592941</td>\n",
       "      <td>-0.270533</td>\n",
       "      <td>0.817739</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.009431</td>\n",
       "      <td>0.798278</td>\n",
       "      <td>-0.137458</td>\n",
       "      <td>0.141267</td>\n",
       "      <td>-0.206010</td>\n",
       "      <td>0.502292</td>\n",
       "      <td>0.219422</td>\n",
       "      <td>0.215153</td>\n",
       "      <td>-0.073813</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>284802</th>\n",
       "      <td>172786.0</td>\n",
       "      <td>-11.881118</td>\n",
       "      <td>10.071785</td>\n",
       "      <td>-9.834783</td>\n",
       "      <td>-2.066656</td>\n",
       "      <td>-5.364473</td>\n",
       "      <td>-2.606837</td>\n",
       "      <td>-4.918215</td>\n",
       "      <td>7.305334</td>\n",
       "      <td>1.914428</td>\n",
       "      <td>...</td>\n",
       "      <td>0.213454</td>\n",
       "      <td>0.111864</td>\n",
       "      <td>1.014480</td>\n",
       "      <td>-0.509348</td>\n",
       "      <td>1.436807</td>\n",
       "      <td>0.250034</td>\n",
       "      <td>0.943651</td>\n",
       "      <td>0.823731</td>\n",
       "      <td>-0.350252</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>284803</th>\n",
       "      <td>172787.0</td>\n",
       "      <td>-0.732789</td>\n",
       "      <td>-0.055080</td>\n",
       "      <td>2.035030</td>\n",
       "      <td>-0.738589</td>\n",
       "      <td>0.868229</td>\n",
       "      <td>1.058415</td>\n",
       "      <td>0.024330</td>\n",
       "      <td>0.294869</td>\n",
       "      <td>0.584800</td>\n",
       "      <td>...</td>\n",
       "      <td>0.214205</td>\n",
       "      <td>0.924384</td>\n",
       "      <td>0.012463</td>\n",
       "      <td>-1.016226</td>\n",
       "      <td>-0.606624</td>\n",
       "      <td>-0.395255</td>\n",
       "      <td>0.068472</td>\n",
       "      <td>-0.053527</td>\n",
       "      <td>-0.254325</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>284804</th>\n",
       "      <td>172788.0</td>\n",
       "      <td>1.919565</td>\n",
       "      <td>-0.301254</td>\n",
       "      <td>-3.249640</td>\n",
       "      <td>-0.557828</td>\n",
       "      <td>2.630515</td>\n",
       "      <td>3.031260</td>\n",
       "      <td>-0.296827</td>\n",
       "      <td>0.708417</td>\n",
       "      <td>0.432454</td>\n",
       "      <td>...</td>\n",
       "      <td>0.232045</td>\n",
       "      <td>0.578229</td>\n",
       "      <td>-0.037501</td>\n",
       "      <td>0.640134</td>\n",
       "      <td>0.265745</td>\n",
       "      <td>-0.087371</td>\n",
       "      <td>0.004455</td>\n",
       "      <td>-0.026561</td>\n",
       "      <td>-0.082239</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>284805</th>\n",
       "      <td>172788.0</td>\n",
       "      <td>-0.240440</td>\n",
       "      <td>0.530483</td>\n",
       "      <td>0.702510</td>\n",
       "      <td>0.689799</td>\n",
       "      <td>-0.377961</td>\n",
       "      <td>0.623708</td>\n",
       "      <td>-0.686180</td>\n",
       "      <td>0.679145</td>\n",
       "      <td>0.392087</td>\n",
       "      <td>...</td>\n",
       "      <td>0.265245</td>\n",
       "      <td>0.800049</td>\n",
       "      <td>-0.163298</td>\n",
       "      <td>0.123205</td>\n",
       "      <td>-0.569159</td>\n",
       "      <td>0.546668</td>\n",
       "      <td>0.108821</td>\n",
       "      <td>0.104533</td>\n",
       "      <td>-0.313391</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>284806</th>\n",
       "      <td>172792.0</td>\n",
       "      <td>-0.533413</td>\n",
       "      <td>-0.189733</td>\n",
       "      <td>0.703337</td>\n",
       "      <td>-0.506271</td>\n",
       "      <td>-0.012546</td>\n",
       "      <td>-0.649617</td>\n",
       "      <td>1.577006</td>\n",
       "      <td>-0.414650</td>\n",
       "      <td>0.486180</td>\n",
       "      <td>...</td>\n",
       "      <td>0.261057</td>\n",
       "      <td>0.643078</td>\n",
       "      <td>0.376777</td>\n",
       "      <td>0.008797</td>\n",
       "      <td>-0.473649</td>\n",
       "      <td>-0.818267</td>\n",
       "      <td>-0.002415</td>\n",
       "      <td>0.013649</td>\n",
       "      <td>0.513290</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>283726 rows × 31 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "            Time         V1         V2        V3        V4        V5  \\\n",
       "0            0.0  -1.359807  -0.072781  2.536347  1.378155 -0.338321   \n",
       "1            0.0   1.191857   0.266151  0.166480  0.448154  0.060018   \n",
       "2            1.0  -1.358354  -1.340163  1.773209  0.379780 -0.503198   \n",
       "3            1.0  -0.966272  -0.185226  1.792993 -0.863291 -0.010309   \n",
       "4            2.0  -1.158233   0.877737  1.548718  0.403034 -0.407193   \n",
       "...          ...        ...        ...       ...       ...       ...   \n",
       "284802  172786.0 -11.881118  10.071785 -9.834783 -2.066656 -5.364473   \n",
       "284803  172787.0  -0.732789  -0.055080  2.035030 -0.738589  0.868229   \n",
       "284804  172788.0   1.919565  -0.301254 -3.249640 -0.557828  2.630515   \n",
       "284805  172788.0  -0.240440   0.530483  0.702510  0.689799 -0.377961   \n",
       "284806  172792.0  -0.533413  -0.189733  0.703337 -0.506271 -0.012546   \n",
       "\n",
       "              V6        V7        V8        V9  ...       V21       V22  \\\n",
       "0       0.462388  0.239599  0.098698  0.363787  ... -0.018307  0.277838   \n",
       "1      -0.082361 -0.078803  0.085102 -0.255425  ... -0.225775 -0.638672   \n",
       "2       1.800499  0.791461  0.247676 -1.514654  ...  0.247998  0.771679   \n",
       "3       1.247203  0.237609  0.377436 -1.387024  ... -0.108300  0.005274   \n",
       "4       0.095921  0.592941 -0.270533  0.817739  ... -0.009431  0.798278   \n",
       "...          ...       ...       ...       ...  ...       ...       ...   \n",
       "284802 -2.606837 -4.918215  7.305334  1.914428  ...  0.213454  0.111864   \n",
       "284803  1.058415  0.024330  0.294869  0.584800  ...  0.214205  0.924384   \n",
       "284804  3.031260 -0.296827  0.708417  0.432454  ...  0.232045  0.578229   \n",
       "284805  0.623708 -0.686180  0.679145  0.392087  ...  0.265245  0.800049   \n",
       "284806 -0.649617  1.577006 -0.414650  0.486180  ...  0.261057  0.643078   \n",
       "\n",
       "             V23       V24       V25       V26       V27       V28    Amount  \\\n",
       "0      -0.110474  0.066928  0.128539 -0.189115  0.133558 -0.021053  0.244200   \n",
       "1       0.101288 -0.339846  0.167170  0.125895 -0.008983  0.014724 -0.342584   \n",
       "2       0.909412 -0.689281 -0.327642 -0.139097 -0.055353 -0.059752  1.158900   \n",
       "3      -0.190321 -1.175575  0.647376 -0.221929  0.062723  0.061458  0.139886   \n",
       "4      -0.137458  0.141267 -0.206010  0.502292  0.219422  0.215153 -0.073813   \n",
       "...          ...       ...       ...       ...       ...       ...       ...   \n",
       "284802  1.014480 -0.509348  1.436807  0.250034  0.943651  0.823731 -0.350252   \n",
       "284803  0.012463 -1.016226 -0.606624 -0.395255  0.068472 -0.053527 -0.254325   \n",
       "284804 -0.037501  0.640134  0.265745 -0.087371  0.004455 -0.026561 -0.082239   \n",
       "284805 -0.163298  0.123205 -0.569159  0.546668  0.108821  0.104533 -0.313391   \n",
       "284806  0.376777  0.008797 -0.473649 -0.818267 -0.002415  0.013649  0.513290   \n",
       "\n",
       "        Class  \n",
       "0           0  \n",
       "1           0  \n",
       "2           0  \n",
       "3           0  \n",
       "4           0  \n",
       "...       ...  \n",
       "284802      0  \n",
       "284803      0  \n",
       "284804      0  \n",
       "284805      0  \n",
       "284806      0  \n",
       "\n",
       "[283726 rows x 31 columns]"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def fearture_scaling(df,col):\n",
    "    features = df[col]\n",
    "    scaler = StandardScaler().fit(features.values.reshape(-1,1))\n",
    "    features = scaler.transform(features.values.reshape(-1,1))\n",
    "    df[col] = features\n",
    "\n",
    "    return df\n",
    "fearture_scaling(df,col = 'Amount')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Train test split\n",
    "Stratified splitting means that when you generate a training / validation dataset split, it will attempt to keep the same percentages of classes in each split.\n",
    "Below shows to splitting tachnique one without stratify and one with. The percentage of Class is printed to show proportion after splitting. With stratify keeps the class proportion."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = df.drop(['Class'], axis = 1)\n",
    "y = df['Class']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Without stratify"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0    0.998333\n",
      "1    0.001667\n",
      "Name: Class, dtype: float64\n",
      "0    0.998332\n",
      "1    0.001668\n",
      "Name: Class, dtype: float64\n"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "X_train, X_test, y_train, y_test = train_test_split(X,y, test_size= 0.3)\n",
    "print(y_train.value_counts() / len(y_train))\n",
    "print(y_test.value_counts() / len(y_test))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### With stratify"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0    0.998333\n",
      "1    0.001667\n",
      "Name: Class, dtype: float64\n",
      "0    0.998332\n",
      "1    0.001668\n",
      "Name: Class, dtype: float64\n"
     ]
    }
   ],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(X,y,stratify = y, test_size= 0.3)\n",
    "print(y_train.value_counts() / len(y_train))\n",
    "print(y_test.value_counts() / len(y_test))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Baseline model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import StratifiedKFold\n",
    "from sklearn.ensemble import RandomForestClassifier"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Stratified cross validation \n",
    "Cross validation is used to evaluate model \n",
    "1. Shuffle the dataset randomly.\n",
    "2. Split the dataset into k groups\n",
    "3. For each unique group:\n",
    "    * Take the group as a hold out or test data set\n",
    "    * Take the remaining groups as a training data set\n",
    "    * Fit a model on the training set and evaluate it on the test set\n",
    "    * Retain the evaluation score and discard the model\n",
    "4. Summarize the skill of the model using the sample of model evaluation scores\n",
    "\n",
    "Note: The training happens mutiple times, the model is the same but model parameter is trained for every loop. It can be used for hyper parameter search or for model selection, to compare if SVM, Logitsic regression, or Radom forest is the better algo for a certain task.\n",
    "\n",
    "Using the stratified K fold technqiue makes sure the class proportion stays the same as the whole dataset inside each fold."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [],
   "source": [
    "stratified_kf = StratifiedKFold(n_splits=5, shuffle = False)\n",
    "rf = RandomForestClassifier(n_estimators=50,random_state=42)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Metrics\n",
    "recall (Sensitivity): TP / (TP + FN) -> The number of positives that were corretly classified over all positive cases. How sentitive is the model to positive cases. Recall should be used when having false negatives is more costly than having false positive. In this use case of detecting credit card fraud, It's more important to catch a fraud case when it truly is fraud, which means the cost of having a false negative is much higher than having a false positive (classifying a case as fraud when it is not).\n",
    "\n",
    "precision: TP / (TP + FP) -> The number of positives that were corretly classified over all cases that were classified as positive. How correct our model is when it predicts a case to be positive. Precision should be used when having false positives is more costly than having false negatives. EX: In a YouTube recommendation system, educing the number of false positives is of utmost importance. False positives here represent videos that the user does not like, but YouTube is still recommending them. False negatives are of lesser importance here since the YouTube recommendations should only contain videos that the user is more likely to click on. If the user sees recommendations that are not of their liking, they will close the application, which is not what YouTube desires. Most automated marketing campaigns require a high precision value to ensure that a large number of potential customers will interact with their survey or be interested to learn more. Or an easier example would be in a zombie apocalypse. You would try to accept as many healthy people as you can into your safe zone, but you really dont want to mistakenly pass a zombie. The true positive is this case is a healthy person and false positive a zombie. It is more important to avoid zombies than accepting more healthy people. So you create a method that causes some of the healthy people mistakenly not to get into the safe zone, but this is the cost of not letting a zombie in.\n",
    "\n",
    "F1 score: 2(precision*recall / (precision + recall)) -> F1 score is the harmonic mean of precision and recall, and is used when false negatives and false positves are eqaully costly. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import cross_val_score\n",
    "score = cross_val_score(rf,X_train,y_train,cv=stratified_kf,scoring='recall')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cross Validation Recall scores are: [0.78787879 0.71212121 0.80597015 0.72727273 0.8030303 ]\n",
      "Average Cross Validation Recall scores is: 0.7672546359113523\n"
     ]
    }
   ],
   "source": [
    "print(f'Cross Validation Recall scores are: {score}')\n",
    "print(f'Average Cross Validation Recall scores is: {score.mean()}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Hyperparameter search using GridsearchCV"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import GridSearchCV"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [],
   "source": [
    "params = {\n",
    "    'n_estimators':[50,100],\n",
    "    # 'max_depth':[4,8]\n",
    "}\n",
    "grid_rf = GridSearchCV(rf, param_grid=params,cv = stratified_kf,scoring='recall').fit(X_train,y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best parameters: {'n_estimators': 50}\n",
      "Best score: 0.7945725915875169\n"
     ]
    }
   ],
   "source": [
    "print('Best parameters:', grid_rf.best_params_)\n",
    "print('Best score:', grid_rf.best_score_)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Test and evaluate model on y_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [],
   "source": [
    "from  sklearn.metrics import confusion_matrix, recall_score, precision_score, f1_score, accuracy_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_hat = grid_rf.predict(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [],
   "source": [
    "cm = confusion_matrix(y_test,y_hat)\n",
    "rf_Recall = recall_score(y_test, y_hat)\n",
    "rf_Precision = precision_score(y_test, y_hat)\n",
    "rf_f1 = f1_score(y_test, y_hat)\n",
    "rf_accuracy = accuracy_score(y_test, y_hat)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Random Forest with</th>\n",
       "      <th>Recall</th>\n",
       "      <th>Precision</th>\n",
       "      <th>F1 Score</th>\n",
       "      <th>Accuracy</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>No Under/Oversampling</td>\n",
       "      <td>0.704225</td>\n",
       "      <td>0.961538</td>\n",
       "      <td>0.813008</td>\n",
       "      <td>0.99946</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "      Random Forest with    Recall  Precision  F1 Score  Accuracy\n",
       "0  No Under/Oversampling  0.704225   0.961538  0.813008   0.99946"
      ]
     },
     "execution_count": 76,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ndf = [(rf_Recall, rf_Precision, rf_f1, rf_accuracy)]\n",
    "\n",
    "rf_score = pd.DataFrame(data = ndf, columns=['Recall','Precision','F1 Score', 'Accuracy'])\n",
    "rf_score.insert(0, 'Random Forest with', 'No Under/Oversampling')\n",
    "rf_score"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "From the result, we can see that the Accuracy is extremely high, but this is due to our imbalaced dataset. Since in our use case (detecting credict cardfraud), having false negative (classifying fraud as not fraud) is more costly than having false positive(classyfing not fraud as fraud),we will use  recall score as the evaluation metric.\n",
    "We will try to beat the baseline model 70.4% recall score in following models."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Resample\n",
    "Two technqiues will be done to resample the dataset.\n",
    "- Oversampling: Randomly duplicate examples in the minority class.\n",
    "- Undersampling: Randomly delete examples in the majority class.\n",
    "\n",
    "**Change to the class distribution should be only applied to the training dataset. The intent is to influence the fit of the models. The resampling is not applied to the test or holdout dataset used to evaluate the performance of a model.**\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "from imblearn.over_sampling import RandomOverSampler\n",
    "from imblearn.under_sampling import RandomUnderSampler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "random_os = RandomOverSampler()\n",
    "random_us = RandomUnderSampler()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_over, y_Over = random_os.fit_resample(X_train,y_train)\n",
    "# The fitted data only contains the traninig data and not the testing\n",
    "X_under, y_under = random_us.fit_resample(X_train,y_train)\n",
    "# The fitted data only contains the traninig data and not the testing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Oversampling:\n",
      "Genuine class count: 198277\n",
      "Fraud class count: 198277\n",
      "0    0.5\n",
      "1    0.5\n",
      "Name: Class, dtype: float64\n",
      "==================================================================\n",
      "Undersampling:\n",
      "Genuine class count: 331\n",
      "Fraud class count: 331\n",
      "0    0.5\n",
      "1    0.5\n",
      "Name: Class, dtype: float64\n",
      "Note: after indersampling the amount of data left to train is very few, not ideal.\n"
     ]
    }
   ],
   "source": [
    "print('Oversampling:')\n",
    "print(f'Genuine class count: {y_Over.value_counts()[0]}')\n",
    "print(f'Fraud class count: {y_Over.value_counts()[1]}')\n",
    "print(y_Over.value_counts() / len(y_Over))\n",
    "print('==================================================================')\n",
    "print('Undersampling:')\n",
    "print(f'Genuine class count: {y_under.value_counts()[0]}')\n",
    "print(f'Fraud class count: {y_under.value_counts()[1]}')\n",
    "print(y_under.value_counts() / len(y_under))\n",
    "print('Note: after indersampling the amount of data left to train is very few, not ideal.')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Building pipeline\n",
    "5 fold srtratified Cross validattion will split the entire dataset into 5 section with same proportion of classes. Every fold will take turns being the validation set and the training segment will gp through the steps bellow. \n",
    "- Oversample / Undersample the minority / majority class\n",
    "- Train the classifier on the training segment (The oversampled or Undersampled part)\n",
    "- Validate the classifier with the remaining segment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "from imblearn.pipeline import Pipeline, make_pipeline "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [],
   "source": [
    "random_overs_pipeline = make_pipeline(RandomOverSampler(),RandomForestClassifier(n_estimators = 50,random_state=42))\n",
    "random_unders_pipeline = make_pipeline(RandomUnderSampler(),RandomForestClassifier(n_estimators = 50,random_state=42))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cross Validation Recall Scores are: [0.75757576 0.68181818 0.82089552 0.75757576 0.8030303 ]\n",
      "Average Cross Validation Recall score: 0.7641791044776118\n",
      "Cross Validation Recall Scores are: [0.72727273 0.6969697  0.82089552 0.75757576 0.8030303 ]\n",
      "Average Cross Validation Recall score: 0.7611488014473089\n"
     ]
    }
   ],
   "source": [
    "score2 = cross_val_score(random_overs_pipeline, X_train, y_train, scoring='recall', cv=stratified_kf)\n",
    "print(\"Cross Validation Recall Scores are: {}\".format(score2))\n",
    "print(\"Average Cross Validation Recall score: {}\".format(score2.mean()))\n",
    "\n",
    "score3 = cross_val_score(random_overs_pipeline, X_train, y_train, scoring='recall', cv=stratified_kf)\n",
    "print(\"Cross Validation Recall Scores are: {}\".format(score3))\n",
    "print(\"Average Cross Validation Recall score: {}\".format(score3.mean()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<style>#sk-container-id-1 {color: black;}#sk-container-id-1 pre{padding: 0;}#sk-container-id-1 div.sk-toggleable {background-color: white;}#sk-container-id-1 label.sk-toggleable__label {cursor: pointer;display: block;width: 100%;margin-bottom: 0;padding: 0.3em;box-sizing: border-box;text-align: center;}#sk-container-id-1 label.sk-toggleable__label-arrow:before {content: \"▸\";float: left;margin-right: 0.25em;color: #696969;}#sk-container-id-1 label.sk-toggleable__label-arrow:hover:before {color: black;}#sk-container-id-1 div.sk-estimator:hover label.sk-toggleable__label-arrow:before {color: black;}#sk-container-id-1 div.sk-toggleable__content {max-height: 0;max-width: 0;overflow: hidden;text-align: left;background-color: #f0f8ff;}#sk-container-id-1 div.sk-toggleable__content pre {margin: 0.2em;color: black;border-radius: 0.25em;background-color: #f0f8ff;}#sk-container-id-1 input.sk-toggleable__control:checked~div.sk-toggleable__content {max-height: 200px;max-width: 100%;overflow: auto;}#sk-container-id-1 input.sk-toggleable__control:checked~label.sk-toggleable__label-arrow:before {content: \"▾\";}#sk-container-id-1 div.sk-estimator input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-1 div.sk-label input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-1 input.sk-hidden--visually {border: 0;clip: rect(1px 1px 1px 1px);clip: rect(1px, 1px, 1px, 1px);height: 1px;margin: -1px;overflow: hidden;padding: 0;position: absolute;width: 1px;}#sk-container-id-1 div.sk-estimator {font-family: monospace;background-color: #f0f8ff;border: 1px dotted black;border-radius: 0.25em;box-sizing: border-box;margin-bottom: 0.5em;}#sk-container-id-1 div.sk-estimator:hover {background-color: #d4ebff;}#sk-container-id-1 div.sk-parallel-item::after {content: \"\";width: 100%;border-bottom: 1px solid gray;flex-grow: 1;}#sk-container-id-1 div.sk-label:hover label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-1 div.sk-serial::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: 0;}#sk-container-id-1 div.sk-serial {display: flex;flex-direction: column;align-items: center;background-color: white;padding-right: 0.2em;padding-left: 0.2em;position: relative;}#sk-container-id-1 div.sk-item {position: relative;z-index: 1;}#sk-container-id-1 div.sk-parallel {display: flex;align-items: stretch;justify-content: center;background-color: white;position: relative;}#sk-container-id-1 div.sk-item::before, #sk-container-id-1 div.sk-parallel-item::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: -1;}#sk-container-id-1 div.sk-parallel-item {display: flex;flex-direction: column;z-index: 1;position: relative;background-color: white;}#sk-container-id-1 div.sk-parallel-item:first-child::after {align-self: flex-end;width: 50%;}#sk-container-id-1 div.sk-parallel-item:last-child::after {align-self: flex-start;width: 50%;}#sk-container-id-1 div.sk-parallel-item:only-child::after {width: 0;}#sk-container-id-1 div.sk-dashed-wrapped {border: 1px dashed gray;margin: 0 0.4em 0.5em 0.4em;box-sizing: border-box;padding-bottom: 0.4em;background-color: white;}#sk-container-id-1 div.sk-label label {font-family: monospace;font-weight: bold;display: inline-block;line-height: 1.2em;}#sk-container-id-1 div.sk-label-container {text-align: center;}#sk-container-id-1 div.sk-container {/* jupyter's `normalize.less` sets `[hidden] { display: none; }` but bootstrap.min.css set `[hidden] { display: none !important; }` so we also need the `!important` here to be able to override the default hidden behavior on the sphinx rendered scikit-learn.org. See: https://github.com/scikit-learn/scikit-learn/issues/21755 */display: inline-block !important;position: relative;}#sk-container-id-1 div.sk-text-repr-fallback {display: none;}</style><div id=\"sk-container-id-1\" class=\"sk-top-container\"><div class=\"sk-text-repr-fallback\"><pre>GridSearchCV(cv=StratifiedKFold(n_splits=5, random_state=None, shuffle=False),\n",
       "             estimator=Pipeline(steps=[(&#x27;randomoversampler&#x27;,\n",
       "                                        RandomOverSampler()),\n",
       "                                       (&#x27;randomforestclassifier&#x27;,\n",
       "                                        RandomForestClassifier(n_estimators=50,\n",
       "                                                               random_state=42))]),\n",
       "             param_grid={&#x27;randomforestclassifier__n_estimators&#x27;: [50, 100]},\n",
       "             return_train_score=True, scoring=&#x27;recall&#x27;)</pre><b>In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. <br />On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.</b></div><div class=\"sk-container\" hidden><div class=\"sk-item sk-dashed-wrapped\"><div class=\"sk-label-container\"><div class=\"sk-label sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-1\" type=\"checkbox\" ><label for=\"sk-estimator-id-1\" class=\"sk-toggleable__label sk-toggleable__label-arrow\">GridSearchCV</label><div class=\"sk-toggleable__content\"><pre>GridSearchCV(cv=StratifiedKFold(n_splits=5, random_state=None, shuffle=False),\n",
       "             estimator=Pipeline(steps=[(&#x27;randomoversampler&#x27;,\n",
       "                                        RandomOverSampler()),\n",
       "                                       (&#x27;randomforestclassifier&#x27;,\n",
       "                                        RandomForestClassifier(n_estimators=50,\n",
       "                                                               random_state=42))]),\n",
       "             param_grid={&#x27;randomforestclassifier__n_estimators&#x27;: [50, 100]},\n",
       "             return_train_score=True, scoring=&#x27;recall&#x27;)</pre></div></div></div><div class=\"sk-parallel\"><div class=\"sk-parallel-item\"><div class=\"sk-item\"><div class=\"sk-label-container\"><div class=\"sk-label sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-2\" type=\"checkbox\" ><label for=\"sk-estimator-id-2\" class=\"sk-toggleable__label sk-toggleable__label-arrow\">estimator: Pipeline</label><div class=\"sk-toggleable__content\"><pre>Pipeline(steps=[(&#x27;randomoversampler&#x27;, RandomOverSampler()),\n",
       "                (&#x27;randomforestclassifier&#x27;,\n",
       "                 RandomForestClassifier(n_estimators=50, random_state=42))])</pre></div></div></div><div class=\"sk-serial\"><div class=\"sk-item\"><div class=\"sk-serial\"><div class=\"sk-item\"><div class=\"sk-estimator sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-3\" type=\"checkbox\" ><label for=\"sk-estimator-id-3\" class=\"sk-toggleable__label sk-toggleable__label-arrow\">RandomOverSampler</label><div class=\"sk-toggleable__content\"><pre>RandomOverSampler()</pre></div></div></div><div class=\"sk-item\"><div class=\"sk-estimator sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-4\" type=\"checkbox\" ><label for=\"sk-estimator-id-4\" class=\"sk-toggleable__label sk-toggleable__label-arrow\">RandomForestClassifier</label><div class=\"sk-toggleable__content\"><pre>RandomForestClassifier(n_estimators=50, random_state=42)</pre></div></div></div></div></div></div></div></div></div></div></div></div>"
      ],
      "text/plain": [
       "GridSearchCV(cv=StratifiedKFold(n_splits=5, random_state=None, shuffle=False),\n",
       "             estimator=Pipeline(steps=[('randomoversampler',\n",
       "                                        RandomOverSampler()),\n",
       "                                       ('randomforestclassifier',\n",
       "                                        RandomForestClassifier(n_estimators=50,\n",
       "                                                               random_state=42))]),\n",
       "             param_grid={'randomforestclassifier__n_estimators': [50, 100]},\n",
       "             return_train_score=True, scoring='recall')"
      ]
     },
     "execution_count": 75,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "new_params = {'randomforestclassifier__' + key: params[key] for key in params}\n",
    "\n",
    "grid_over_rf = GridSearchCV(random_overs_pipeline, param_grid=new_params, cv=stratified_kf, scoring='recall',\n",
    "                        return_train_score=True)\n",
    "grid_over_rf.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best parameters: {'randomforestclassifier__n_estimators': 50}\n",
      "Best score: 0.7611488014473089\n"
     ]
    }
   ],
   "source": [
    "print('Best parameters:', grid_over_rf.best_params_)\n",
    "print('Best score:', grid_over_rf.best_score_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_hat = grid_over_rf.best_estimator_.named_steps['randomforestclassifier'].predict(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[84967     9]\n",
      " [   31   111]]\n"
     ]
    }
   ],
   "source": [
    "cm = confusion_matrix(y_test, y_hat)\n",
    "\n",
    "over_rf_Recall = recall_score(y_test, y_hat)\n",
    "over_rf_Precision = precision_score(y_test, y_hat)\n",
    "over_rf_f1 = f1_score(y_test, y_hat)\n",
    "over_rf_accuracy = accuracy_score(y_test, y_hat)\n",
    "\n",
    "print(cm)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Random Forest with</th>\n",
       "      <th>Recall</th>\n",
       "      <th>Precision</th>\n",
       "      <th>F1 Score</th>\n",
       "      <th>Accuracy</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Random Oversampling</td>\n",
       "      <td>0.78169</td>\n",
       "      <td>0.925</td>\n",
       "      <td>0.847328</td>\n",
       "      <td>0.99953</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "    Random Forest with   Recall  Precision  F1 Score  Accuracy\n",
       "0  Random Oversampling  0.78169      0.925  0.847328   0.99953"
      ]
     },
     "execution_count": 79,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ndf = [(over_rf_Recall, over_rf_Precision, over_rf_f1, over_rf_accuracy)]\n",
    "\n",
    "over_rf_score = pd.DataFrame(data = ndf, columns=['Recall','Precision','F1 Score', 'Accuracy'])\n",
    "over_rf_score.insert(0, 'Random Forest with', 'Random Oversampling')\n",
    "over_rf_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<style>#sk-container-id-2 {color: black;}#sk-container-id-2 pre{padding: 0;}#sk-container-id-2 div.sk-toggleable {background-color: white;}#sk-container-id-2 label.sk-toggleable__label {cursor: pointer;display: block;width: 100%;margin-bottom: 0;padding: 0.3em;box-sizing: border-box;text-align: center;}#sk-container-id-2 label.sk-toggleable__label-arrow:before {content: \"▸\";float: left;margin-right: 0.25em;color: #696969;}#sk-container-id-2 label.sk-toggleable__label-arrow:hover:before {color: black;}#sk-container-id-2 div.sk-estimator:hover label.sk-toggleable__label-arrow:before {color: black;}#sk-container-id-2 div.sk-toggleable__content {max-height: 0;max-width: 0;overflow: hidden;text-align: left;background-color: #f0f8ff;}#sk-container-id-2 div.sk-toggleable__content pre {margin: 0.2em;color: black;border-radius: 0.25em;background-color: #f0f8ff;}#sk-container-id-2 input.sk-toggleable__control:checked~div.sk-toggleable__content {max-height: 200px;max-width: 100%;overflow: auto;}#sk-container-id-2 input.sk-toggleable__control:checked~label.sk-toggleable__label-arrow:before {content: \"▾\";}#sk-container-id-2 div.sk-estimator input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-2 div.sk-label input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-2 input.sk-hidden--visually {border: 0;clip: rect(1px 1px 1px 1px);clip: rect(1px, 1px, 1px, 1px);height: 1px;margin: -1px;overflow: hidden;padding: 0;position: absolute;width: 1px;}#sk-container-id-2 div.sk-estimator {font-family: monospace;background-color: #f0f8ff;border: 1px dotted black;border-radius: 0.25em;box-sizing: border-box;margin-bottom: 0.5em;}#sk-container-id-2 div.sk-estimator:hover {background-color: #d4ebff;}#sk-container-id-2 div.sk-parallel-item::after {content: \"\";width: 100%;border-bottom: 1px solid gray;flex-grow: 1;}#sk-container-id-2 div.sk-label:hover label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-2 div.sk-serial::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: 0;}#sk-container-id-2 div.sk-serial {display: flex;flex-direction: column;align-items: center;background-color: white;padding-right: 0.2em;padding-left: 0.2em;position: relative;}#sk-container-id-2 div.sk-item {position: relative;z-index: 1;}#sk-container-id-2 div.sk-parallel {display: flex;align-items: stretch;justify-content: center;background-color: white;position: relative;}#sk-container-id-2 div.sk-item::before, #sk-container-id-2 div.sk-parallel-item::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: -1;}#sk-container-id-2 div.sk-parallel-item {display: flex;flex-direction: column;z-index: 1;position: relative;background-color: white;}#sk-container-id-2 div.sk-parallel-item:first-child::after {align-self: flex-end;width: 50%;}#sk-container-id-2 div.sk-parallel-item:last-child::after {align-self: flex-start;width: 50%;}#sk-container-id-2 div.sk-parallel-item:only-child::after {width: 0;}#sk-container-id-2 div.sk-dashed-wrapped {border: 1px dashed gray;margin: 0 0.4em 0.5em 0.4em;box-sizing: border-box;padding-bottom: 0.4em;background-color: white;}#sk-container-id-2 div.sk-label label {font-family: monospace;font-weight: bold;display: inline-block;line-height: 1.2em;}#sk-container-id-2 div.sk-label-container {text-align: center;}#sk-container-id-2 div.sk-container {/* jupyter's `normalize.less` sets `[hidden] { display: none; }` but bootstrap.min.css set `[hidden] { display: none !important; }` so we also need the `!important` here to be able to override the default hidden behavior on the sphinx rendered scikit-learn.org. See: https://github.com/scikit-learn/scikit-learn/issues/21755 */display: inline-block !important;position: relative;}#sk-container-id-2 div.sk-text-repr-fallback {display: none;}</style><div id=\"sk-container-id-2\" class=\"sk-top-container\"><div class=\"sk-text-repr-fallback\"><pre>GridSearchCV(cv=StratifiedKFold(n_splits=5, random_state=None, shuffle=False),\n",
       "             estimator=Pipeline(steps=[(&#x27;randomundersampler&#x27;,\n",
       "                                        RandomUnderSampler()),\n",
       "                                       (&#x27;randomforestclassifier&#x27;,\n",
       "                                        RandomForestClassifier(n_estimators=50,\n",
       "                                                               random_state=42))]),\n",
       "             param_grid={&#x27;randomforestclassifier__n_estimators&#x27;: [50, 100]},\n",
       "             return_train_score=True, scoring=&#x27;recall&#x27;)</pre><b>In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. <br />On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.</b></div><div class=\"sk-container\" hidden><div class=\"sk-item sk-dashed-wrapped\"><div class=\"sk-label-container\"><div class=\"sk-label sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-5\" type=\"checkbox\" ><label for=\"sk-estimator-id-5\" class=\"sk-toggleable__label sk-toggleable__label-arrow\">GridSearchCV</label><div class=\"sk-toggleable__content\"><pre>GridSearchCV(cv=StratifiedKFold(n_splits=5, random_state=None, shuffle=False),\n",
       "             estimator=Pipeline(steps=[(&#x27;randomundersampler&#x27;,\n",
       "                                        RandomUnderSampler()),\n",
       "                                       (&#x27;randomforestclassifier&#x27;,\n",
       "                                        RandomForestClassifier(n_estimators=50,\n",
       "                                                               random_state=42))]),\n",
       "             param_grid={&#x27;randomforestclassifier__n_estimators&#x27;: [50, 100]},\n",
       "             return_train_score=True, scoring=&#x27;recall&#x27;)</pre></div></div></div><div class=\"sk-parallel\"><div class=\"sk-parallel-item\"><div class=\"sk-item\"><div class=\"sk-label-container\"><div class=\"sk-label sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-6\" type=\"checkbox\" ><label for=\"sk-estimator-id-6\" class=\"sk-toggleable__label sk-toggleable__label-arrow\">estimator: Pipeline</label><div class=\"sk-toggleable__content\"><pre>Pipeline(steps=[(&#x27;randomundersampler&#x27;, RandomUnderSampler()),\n",
       "                (&#x27;randomforestclassifier&#x27;,\n",
       "                 RandomForestClassifier(n_estimators=50, random_state=42))])</pre></div></div></div><div class=\"sk-serial\"><div class=\"sk-item\"><div class=\"sk-serial\"><div class=\"sk-item\"><div class=\"sk-estimator sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-7\" type=\"checkbox\" ><label for=\"sk-estimator-id-7\" class=\"sk-toggleable__label sk-toggleable__label-arrow\">RandomUnderSampler</label><div class=\"sk-toggleable__content\"><pre>RandomUnderSampler()</pre></div></div></div><div class=\"sk-item\"><div class=\"sk-estimator sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-8\" type=\"checkbox\" ><label for=\"sk-estimator-id-8\" class=\"sk-toggleable__label sk-toggleable__label-arrow\">RandomForestClassifier</label><div class=\"sk-toggleable__content\"><pre>RandomForestClassifier(n_estimators=50, random_state=42)</pre></div></div></div></div></div></div></div></div></div></div></div></div>"
      ],
      "text/plain": [
       "GridSearchCV(cv=StratifiedKFold(n_splits=5, random_state=None, shuffle=False),\n",
       "             estimator=Pipeline(steps=[('randomundersampler',\n",
       "                                        RandomUnderSampler()),\n",
       "                                       ('randomforestclassifier',\n",
       "                                        RandomForestClassifier(n_estimators=50,\n",
       "                                                               random_state=42))]),\n",
       "             param_grid={'randomforestclassifier__n_estimators': [50, 100]},\n",
       "             return_train_score=True, scoring='recall')"
      ]
     },
     "execution_count": 80,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "grid_under_rf = GridSearchCV(random_unders_pipeline, param_grid=new_params, cv=stratified_kf, scoring='recall',\n",
    "                        return_train_score=True)\n",
    "grid_under_rf.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best parameters: {'randomforestclassifier__n_estimators': 50}\n",
      "Best score: 0.7611488014473089\n"
     ]
    }
   ],
   "source": [
    "print('Best parameters:', grid_over_rf.best_params_)\n",
    "print('Best score:', grid_over_rf.best_score_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_hat = grid_under_rf.best_estimator_.named_steps['randomforestclassifier'].predict(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[83247  1729]\n",
      " [   15   127]]\n"
     ]
    }
   ],
   "source": [
    "cm = confusion_matrix(y_test, y_hat)\n",
    "\n",
    "under_rf_Recall = recall_score(y_test, y_hat)\n",
    "under_rf_Precision = precision_score(y_test, y_hat)\n",
    "under_rf_f1 = f1_score(y_test, y_hat)\n",
    "under_rf_accuracy = accuracy_score(y_test, y_hat)\n",
    "\n",
    "print(cm)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Random Forest with</th>\n",
       "      <th>Recall</th>\n",
       "      <th>Precision</th>\n",
       "      <th>F1 Score</th>\n",
       "      <th>Accuracy</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Random Undersampling</td>\n",
       "      <td>0.894366</td>\n",
       "      <td>0.068427</td>\n",
       "      <td>0.127127</td>\n",
       "      <td>0.979511</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "     Random Forest with    Recall  Precision  F1 Score  Accuracy\n",
       "0  Random Undersampling  0.894366   0.068427  0.127127  0.979511"
      ]
     },
     "execution_count": 85,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ndf = [(under_rf_Recall, under_rf_Precision, under_rf_f1, under_rf_accuracy)]\n",
    "\n",
    "under_rf_score = pd.DataFrame(data = ndf, columns=['Recall','Precision','F1 Score', 'Accuracy'])\n",
    "under_rf_score.insert(0, 'Random Forest with', 'Random Undersampling')\n",
    "under_rf_score"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# SMOTE(Synthetic Minority Oversampling Technique)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "tf2",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
